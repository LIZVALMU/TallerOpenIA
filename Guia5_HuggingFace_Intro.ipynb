{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad982ac",
   "metadata": {},
   "source": [
    "# Guía — Introducción práctica a Hugging Face (HF)\n",
    "Curso: **IA en el Aula — Nivel Avanzado**  \n",
    "Autor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **Octubre 2025**\n",
    "\n",
    "Esta guía introduce el ecosistema de **Hugging Face**: conceptos, librerías principales, opciones de ejecución (local vs remoto) y ejemplos de código en Python. Incluye buenas prácticas, notas de licencias y consideraciones éticas en educación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b28a60",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Hugging Face?\n",
    "- **Hugging Face (HF)** es un ecosistema abierto para **modelos de aprendizaje automático**, datasets, espacios de demostración y herramientas.\n",
    "- Componentes clave:\n",
    "  - **Hugging Face Hub**: repositorio colaborativo de modelos, datasets y Spaces.\n",
    "  - **Librerías** (desarrolladas por **Hugging Face y su comunidad**):\n",
    "    - **transformers**: modelos de NLP/visión/audio (state-of-the-art) y utilidades de inferencia/entrenamiento.\n",
    "    - **datasets**: carga/limpieza/streaming de conjuntos de datos.\n",
    "    - **tokenizers**: tokenización eficiente (Rust/Python) para modelos modernos.\n",
    "    - **accelerate**, **peft**, **trl**: entrenamiento eficiente, fine-tuning ligero (LoRA), RLHF, etc.\n",
    "  - **Spaces**: apps (Gradio/Streamlit) desplegadas en la nube de HF.\n",
    "\n",
    "**¿Quién desarrolla estas librerías?**  \n",
    "Principalmente el **equipo de ingeniería de Hugging Face** junto con una **amplia comunidad open source** (universidades, empresas y desarrolladores independientes) que contribuyen vía pull requests, issues y discusiones. Los repositorios son públicos (licencias abiertas) y cuentan con mantenedores oficiales de HF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec75e73",
   "metadata": {},
   "source": [
    "## 2. Instalación rápida\n",
    "Ejecuta esta celda para instalar los paquetes base. En entornos gestionados, puedes omitir instalación si ya existen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9874245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alison.valderrama-m.labinfo\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets huggingface_hub tokenizers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb70c2d",
   "metadata": {},
   "source": [
    "## 3. Conceptos esenciales\n",
    "- **Modelo**: pesos entrenados + arquitectura (`AutoModel*`, `AutoTokenizer`).\n",
    "- **Pipeline**: interfaz de alto nivel para tareas (clasificación, generación, QA, embeddings, etc.).\n",
    "- **Cache local**: HF guarda modelos y datasets en `~/.cache/huggingface/` para reutilización.\n",
    "- **Ejecución local vs remota**:\n",
    "  - *Local*: descargas pesos una vez, ejecutas con tu CPU/GPU.\n",
    "  - *Remota*: usas **Inference API** o **Spaces** (HF los ejecuta en su nube; puede haber límites o costos).\n",
    "- **Model Card**: ficha del modelo (uso previsto, limitaciones, licencias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de398b",
   "metadata": {},
   "source": [
    "## 4. Primeros pasos con `transformers` (local)\n",
    "Usaremos `pipeline` para hacer inferencia con modelos ligeros. La primera ejecución descarga los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1315d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ce7a79467640879884a297af21a6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alison.valderrama-m.LABINFO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alison.valderrama-m.LABINFO\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6c74155dac487fa5d1326232c76e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf34d27367cb4e8c9854bc818d4d3429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a6fd026a394dc6bf0b5a2b9f4999d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9524868726730347}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2099886a22594a8889ad81b905e0ec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alison.valderrama-m.LABINFO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alison.valderrama-m.LABINFO\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e306c673698d4829954068df5c0528bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2e3fcd4e11432a8bac12e75d14b818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d012f66a91724379909d484316522218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba047fa1faa84df09e2713166d975588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cc2d59d7be4c24849614448dc7784e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5d142339f94facba9292d6d568ef82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello AI classroom, today we will learn about the basics of AI and how to make sure you are successful.\\n\\nThis course contains:\\n\\nPrerequisites\\n\\nSoftware Development and Training ('}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Clasificación de sentimientos (modelo ligero por defecto)\n",
    "clf = pipeline(\"sentiment-analysis\")\n",
    "print(clf(\"Este curso de IA en el aula me parece excelente.\"))\n",
    "\n",
    "# Generación de texto (modelo base pequeño)\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=30)\n",
    "print(gen(\"Hello AI classroom, today we will learn about\", num_return_sequences=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de99ec2",
   "metadata": {},
   "source": [
    "### Notas\n",
    "- La primera ejecución descargará los pesos (conexión requerida) y los guardará en caché.\n",
    "- En ejecuciones posteriores, se cargan desde disco.\n",
    "- Para usar GPU (si existe), pasa `device=0` al crear el pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba625054",
   "metadata": {},
   "source": [
    "## 5. Uso de la API de AutoModel/AutoTokenizer (más control)\n",
    "Cuando necesites más control que `pipeline`, usa las clases automáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79715852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9544c002c02d47888fc1ddd6b6c83504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alison.valderrama-m.LABINFO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alison.valderrama-m.LABINFO\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25cf9002f444507b8e47e3c02886115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c69675e58aa40c88134f03e01405a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62921658caa4e27ae27179682cf88f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVE': 0.0004911534488201141, 'POSITIVE': 0.9995088577270508}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bff309",
   "metadata": {},
   "source": [
    "### Explicación detallada: Clasificación de sentimientos con Hugging Face\n",
    "\n",
    "Este ejemplo muestra cómo interactuar directamente con un modelo de Hugging Face sin usar `pipeline`, para comprender el flujo interno de tokenización, inferencia y decodificación.\n",
    "\n",
    "---\n",
    "\n",
    "### Importaciones\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "```\n",
    "\n",
    "- `AutoTokenizer`: descarga el **tokenizador** adecuado al modelo (convierte texto a IDs numéricos).  \n",
    "- `AutoModelForSequenceClassification`: carga el **modelo neuronal** y sus **pesos** preentrenados para tareas de **clasificación** (por ejemplo, sentimiento).  \n",
    "- `torch`: motor numérico (PyTorch) que gestiona tensores y operaciones de la red neuronal.\n",
    "\n",
    "---\n",
    "\n",
    "### Seleccionar el modelo\n",
    "\n",
    "```python\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "```\n",
    "\n",
    "- `distilbert`: versión compacta de **BERT** (Distilled BERT).  \n",
    "- `base-uncased`: vocabulario sin distinción de mayúsculas/minúsculas.  \n",
    "- `finetuned-sst-2`: ajustado sobre el dataset **Stanford Sentiment Treebank v2**, especializado en **análisis de sentimiento (positivo/negativo)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Cargar el tokenizador y el modelo\n",
    "\n",
    "```python\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "- `from_pretrained()` busca el modelo en la caché local o lo descarga del **Hugging Face Hub**.  \n",
    "- El paquete incluye pesos, configuración, tokenizador y metadatos.  \n",
    "- `tok` traduce texto → números; `model` ejecuta la red neuronal con esos números.\n",
    "\n",
    "---\n",
    "\n",
    "### Tokenizar el texto\n",
    "\n",
    "```python\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "```\n",
    "\n",
    "El tokenizador convierte la frase en IDs y máscaras de atención:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_ids': tensor([[101, 1045, 2293, 3331, 9935, 4822, 1012, 102]]),\n",
    "    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "```\n",
    "\n",
    "- `input_ids`: IDs numéricos correspondientes a las subpalabras.  \n",
    "- `attention_mask`: marca con 1 las posiciones activas y con 0 el relleno (padding).  \n",
    "- `return_tensors=\"pt\"` devuelve tensores de PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### Inferencia (paso hacia adelante del modelo)\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "```\n",
    "\n",
    "- `torch.no_grad()`: desactiva el cálculo de gradientes (modo inferencia, más eficiente).  \n",
    "- `model(**inputs)`: pasa los tensores por las capas de la red neuronal.  \n",
    "- `.logits`: salida sin normalizar, vector con una puntuación por clase.  \n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "tensor([[-2.13, 3.56]])\n",
    "```\n",
    "→ Puntuación baja para *NEGATIVE*, alta para *POSITIVE*.\n",
    "\n",
    "---\n",
    "\n",
    "### Convertir logits en probabilidades\n",
    "\n",
    "```python\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "```\n",
    "\n",
    "- `softmax`: convierte las puntuaciones en probabilidades que suman 1.  \n",
    "- `dim=-1`: aplica la operación sobre la última dimensión (las clases).  \n",
    "- `tolist()[0]`: transforma el tensor a una lista Python.\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "[0.01, 0.99]\n",
    "```\n",
    "→ 1% negativo, 99% positivo.\n",
    "\n",
    "---\n",
    "\n",
    "### Mostrar el resultado\n",
    "\n",
    "```python\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n",
    "```\n",
    "Salida:\n",
    "```python\n",
    "{'NEGATIVE': 0.0123, 'POSITIVE': 0.9877}\n",
    "```\n",
    "El modelo predice un sentimiento **claramente positivo** para la frase.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen del flujo interno\n",
    "\n",
    "| Etapa | Qué hace | Tipo de dato |\n",
    "|--------|-----------|--------------|\n",
    "| Tokenizer | Convierte texto → IDs | Diccionario de tensores |\n",
    "| Modelo | Procesa los tensores y produce logits | Tensor 2D |\n",
    "| Softmax | Convierte logits → probabilidades | Lista o array |\n",
    "| Salida final | Devuelve un diccionario con clases y probabilidades | Dict |\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptos clave para el aula\n",
    "\n",
    "- **Tokenización:** convierte lenguaje humano a números comprensibles por el modelo.  \n",
    "- **Logits vs Probabilidades:** logits son puntuaciones sin escalar; `softmax` las convierte en probabilidades.  \n",
    "- **Fine-tuning:** ajuste de un modelo base a una tarea específica.  \n",
    "- **Inferencia:** uso del modelo para predecir (sin entrenamiento).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5905a99",
   "metadata": {},
   "source": [
    "## 6. Trabajar con `datasets`\n",
    "La librería `datasets` permite cargar datos públicos del Hub y tratarlos como DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b78ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0746eef187c14532ac18d051e4a64486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alison.valderrama-m.LABINFO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alison.valderrama-m.LABINFO\\.cache\\huggingface\\hub\\datasets--ag_news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2786e105d4407092433655a0ff4cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad94eb3f8c2646d599bd48c3f87f5d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0647e6ba4afc4c39971f755f42ce4aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721939146da142599ee3a5014b42fe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Carga un dataset de ejemplo (pequeño)\n",
    "ds = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(ds)\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b2ff0",
   "metadata": {},
   "source": [
    "## 7. Hugging Face Inference API (remoto)\n",
    "Si no quieres descargar ni ejecutar modelos localmente, puedes llamar a la **Inference API**. Requiere una cuenta HF y, en algunos casos, **token de acceso** (con cuota gratuita limitada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fd6075",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Opcional: configura tu token HF si es necesario\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# os.environ['HF_TOKEN'] = 'hf_...'\u001b[39;00m\n\u001b[32m      7\u001b[39m client = InferenceClient(model=\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# modelo público sencillo\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m out = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello from remote HF Inference API!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\inference\\_client.py:2356\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2350\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAPI endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Please pass `stream=False` as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2353\u001b[39m         )\n\u001b[32m   2355\u001b[39m model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2357\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m   2358\u001b[39m     inputs=prompt,\n\u001b[32m   2359\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2363\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m   2364\u001b[39m )\n\u001b[32m   2366\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\inference\\_providers\\__init__.py:217\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m     provider_mapping = _fetch_inference_provider_mapping(model)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     provider = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprovider_mapping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.provider\n\u001b[32m    219\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "# Opcional: configura tu token HF si es necesario\n",
    "# os.environ['HF_TOKEN'] = 'hf_...'\n",
    "\n",
    "client = InferenceClient(model=\"gpt2\")  # modelo público sencillo\n",
    "out = client.text_generation(\"Hello from remote HF Inference API!\", max_new_tokens=32)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88e0a0",
   "metadata": {},
   "source": [
    "### ¿Cuándo usar local vs remoto?\n",
    "- **Local**: control total, sin costos por uso; requiere recursos (CPU/GPU/RAM) y descarga de pesos.\n",
    "- **Remoto (Inference API/Spaces)**: cero instalación, útil para demos/producción; puede tener límites/costos.\n",
    "- En docencia: comenzar local con modelos pequeños; escalar a remoto si es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bfd94",
   "metadata": {},
   "source": [
    "## 8. Embeddings (representaciones vectoriales)\n",
    "Los embeddings son útiles en **búsqueda semántica**, **RAG** y **análisis de similitud**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3d1ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d528ae9e50d846c894b90834f524de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alison.valderrama-m.LABINFO\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alison.valderrama-m.LABINFO\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e4eb7d32964d1f8b626b4c630b81b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205bf889482c48b880dab1175599b5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab924ee18074d1b9899198d079b25fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c1c3824fb46b9879ec92ec7d33cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641cc7863d494808a408801f93311b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 384])\n",
      "Cosine similarity (0 vs 1): 0.468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "emb_tok = AutoTokenizer.from_pretrained(emb_model_id)\n",
    "emb_model = AutoModel.from_pretrained(emb_model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    # Tokenización con padding/truncado\n",
    "    batch = emb_tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**batch)\n",
    "    # Mean pooling simple sobre la última capa\n",
    "    tokens = out.last_hidden_state  # [batch, seq, hidden]\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)  # [batch, seq, 1]\n",
    "    masked = tokens * mask\n",
    "    sent_emb = masked.sum(dim=1) / mask.sum(dim=1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "e = embed([\"inteligencia artificial en educación\", \"clase de programación\", \"oxigenación en hidroeléctricas\"])\n",
    "print(e.shape)\n",
    "# Similitud coseno entre primera y segunda\n",
    "sim = (e[0] @ e[1]).item()\n",
    "print(\"Cosine similarity (0 vs 1):\", round(sim, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06be84",
   "metadata": {},
   "source": [
    "# Comprendiendo los Embeddings en Inteligencia Artificial\n",
    "\n",
    "## ¿Qué es un *embedding*?\n",
    "\n",
    "Un **embedding** es una **representación numérica** de un dato (texto, imagen, audio, etc.) en un **espacio vectorial continuo**.  \n",
    "En lugar de trabajar directamente con palabras o símbolos, los modelos de IA los transforman en **vectores de números reales**, de modo que conceptos similares queden **cercanos entre sí** en ese espacio.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| Palabra | Embedding (simplificado) |\n",
    "|----------|--------------------------|\n",
    "| \"perro\" | [0.82, 0.10, 0.44, …] |\n",
    "| \"gato\"  | [0.80, 0.12, 0.46, …] |\n",
    "| \"avión\" | [-0.30, 0.90, -0.12, …] |\n",
    "\n",
    "Si calculamos la distancia entre vectores, veremos que **“perro”** y **“gato”** están mucho más cerca que **“perro”** y **“avión”**.  \n",
    "Esto significa que el modelo **captura relaciones semánticas** (de significado) entre palabras.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuición geométrica\n",
    "\n",
    "Los embeddings convierten el lenguaje en **puntos en un espacio N-dimensional**, donde la geometría refleja las relaciones conceptuales:\n",
    "\n",
    "- Distancias pequeñas → conceptos similares.  \n",
    "- Distancias grandes → conceptos distintos.  \n",
    "- A veces incluso se pueden representar relaciones lineales:\n",
    "  ```\n",
    "  vector(\"rey\") - vector(\"hombre\") + vector(\"mujer\") ≈ vector(\"reina\")\n",
    "  ```\n",
    "\n",
    "Por eso los embeddings son la base de muchas tareas de **razonamiento semántico** en IA.\n",
    "\n",
    "---\n",
    "\n",
    "## Cómo se generan los embeddings\n",
    "\n",
    "Los embeddings se **aprenden** durante el entrenamiento de modelos.  \n",
    "En redes neuronales, la primera capa suele ser una **capa de embedding** que asigna a cada palabra un vector.\n",
    "\n",
    "- En modelos clásicos como **Word2Vec**, **GloVe** o **FastText**, los embeddings se entrenan explícitamente observando qué palabras aparecen juntas.\n",
    "- En modelos modernos (**BERT**, **GPT**, **DistilBERT**), los embeddings son el resultado de las **capas internas** de atención.  \n",
    "  Se pueden extraer desde cualquiera de esas capas usando librerías como `transformers`.\n",
    "\n",
    "---\n",
    "\n",
    "## Embeddings en Hugging Face\n",
    "\n",
    "Podemos usar un modelo especializado en *sentence embeddings* (como los de **Sentence Transformers**) para convertir textos completos en vectores comparables:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    batch = tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "    # Promedio (mean pooling)\n",
    "    tokens = out.last_hidden_state\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)\n",
    "    sent_emb = (tokens * mask).sum(1) / mask.sum(1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "emb = embed([\"inteligencia artificial\", \"aprendizaje automático\", \"física cuántica\"])\n",
    "print(emb.shape)\n",
    "```\n",
    "\n",
    "Cada texto se convierte en un vector de dimensión 384 o 768 (según el modelo).\n",
    "\n",
    "---\n",
    "\n",
    "## Comparación entre embeddings\n",
    "\n",
    "Para comparar embeddings se usa la **similitud del coseno**, que mide el ángulo entre los vectores:\n",
    "\n",
    "\\[\n",
    "\\text{similaridad}(A,B) = \\frac{A \\cdot B}{||A|| \\, ||B||}\n",
    "\\]\n",
    "\n",
    "Valores:\n",
    "- 1.0 → textos muy similares.  \n",
    "- 0.0 → no relacionados.  \n",
    "- -1.0 → opuestos conceptualmente.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "sim = float((emb[0] @ emb[1]).item())\n",
    "print(\"Similitud coseno:\", round(sim, 4))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones educativas y prácticas\n",
    "\n",
    "Los embeddings son fundamentales en muchos sistemas modernos:\n",
    "\n",
    "| Aplicación | Uso de embeddings |\n",
    "|-------------|------------------|\n",
    "| **RAG (Retrieval-Augmented Generation)** | Recuperar documentos relevantes antes de generar texto. |\n",
    "| **Búsqueda semántica** | Encontrar textos “parecidos” por significado, no por palabras exactas. |\n",
    "| **Clasificación y clustering** | Agrupar frases o estudiantes por temas o patrones. |\n",
    "| **Recomendadores** | Calcular similitud entre recursos educativos. |\n",
    "| **Análisis de discurso** | Detectar similitudes en respuestas escritas o redacciones. |\n",
    "\n",
    "---\n",
    "\n",
    "## Buenas prácticas\n",
    "\n",
    "- Normaliza los vectores antes de compararlos (`F.normalize` o división por norma).  \n",
    "- Usa embeddings de **oraciones** (Sentence Transformers) para comparar frases completas.  \n",
    "- Almacena embeddings en bases vectoriales como **FAISS** o **Chroma** para búsquedas rápidas.  \n",
    "- Recuerda que los embeddings reflejan el **sesgo** del modelo con el que se entrenaron; úsalo de forma crítica y responsable.\n",
    "\n",
    "---\n",
    "\n",
    "## En resumen\n",
    "\n",
    "> Un *embedding* es una forma matemática de representar significado.  \n",
    "> Convierte información simbólica (palabras, imágenes, sonidos) en **vectores** donde la distancia representa **relación semántica**.  \n",
    "> Son el puente entre el lenguaje humano y el razonamiento numérico de los modelos de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2d8e1",
   "metadata": {},
   "source": [
    "## 9. Tokenización con `tokenizers`\n",
    "`tokenizers` (Rust + Python) es la base de la tokenización rápida y reproducible para modelos modernos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085a68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer BPE de ejemplo creado (demo).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "# Ejemplo mínimo: crear un tokenizer vacío BPE (demostrativo)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# Nota: entrenar un tokenizer real requiere un corpus y procesos adicionales (no cubierto aquí).\n",
    "print(\"Tokenizer BPE de ejemplo creado (demo).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826326e4",
   "metadata": {},
   "source": [
    "## 10. Buenas prácticas, licencias y ética\n",
    "- Revisa la **Model Card** antes de usar un modelo: usos previstos, sesgos, limitaciones.\n",
    "- Respeta las **licencias** de modelos/datasets; algunos son comerciales, otros sólo para investigación.\n",
    "- En el aula: evita datos personales reales, valida salidas y cita fuentes de modelo/dataset.\n",
    "- Optimiza recursos: usa modelos pequeños para clases; documenta versiones y hashes de commit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0a344",
   "metadata": {},
   "source": [
    "## 11. Solución de problemas comunes\n",
    "- **`ModuleNotFoundError`**: reinstala el paquete faltante; reinicia kernel.\n",
    "- **`OSError: Can't load tokenizer/model`**: el ID del modelo es incorrecto o no tienes permisos.\n",
    "- **Falta de RAM/GPU**: usa modelos más pequeños o recurre a Inference API.\n",
    "- **Rate limit (remoto)**: espera o considera un plan de pago.\n",
    "- **Conectividad**: comprueba proxies/firewalls y credenciales (HF_TOKEN si aplica)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219454f3",
   "metadata": {},
   "source": [
    "## 12. Próximos pasos (sugerencias)\n",
    "- Fine-tuning ligero con **PEFT/LoRA** sobre un dataset pequeño.\n",
    "- Construir un mini-**RAG** con embeddings + FAISS/Chroma.\n",
    "- Publicar una demo en **HF Spaces** (Gradio) para compartir con estudiantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello_ai_vscode",
   "language": "python",
   "name": "hello_ai_vscode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
